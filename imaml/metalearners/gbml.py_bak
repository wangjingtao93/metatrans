import numpy as np
import torch
import torch.nn as nn
import os
import torchvision.models
from model.transformer.vit_model import vit_base_patch16_224 
from model.mtb.mtb_model import create_mtb 

# 暂时未找到这俩
# from net.convnet import ConvNet
# from net.resnet import ResNet

# https://github.com/sungyubkim/GBML/blob/master/main.py
class GBML:
    '''
    Gradient-Based Meta-Learning
    '''
    def __init__(self, args):
        self.args = args
        # self.batch_size = self.args.batch_size
        return None

    def _init_net(self):
        if self.args.net == 'ConvNet':
            # self.network = ConvNet(self.args)
            pass
        elif self.args.net == 'resnet18':
            # self.network = ResNet(self.args)
            # self.args.hidden_channels = 640
            self.network = torchvision.models.resnet18(pretrained=False)
            num_ftrs = self.network.fc.in_features
            self.network.fc = nn.Linear(num_ftrs, 4)
        elif self.args.net == 'vit_base_patch16_224':
            self.network = vit_base_patch16_224(num_classes=4)
            self.network.load_state_dict(torch.load('/data1/wangjingtao/workplace/python/pycharm_remote/meta-learning-classfication/pre_pth/vit_base_patch16_224.pth'))
            print('using transformer with pretrain')
            self.freeze_blocks()
        elif self.args.net == 'vit_base_patch16_224_depth_6':
            self.network = vit_base_patch16_224(num_classes=4, depth=6)
            self.network.load_state_dict(torch.load('/data1/wangjingtao/workplace/python/pycharm_remote/meta-learning-classfication/pre_pth/vit_base_patch16_224_depth_6.pth'))
            print('using transformer with pretrain depth_6')  
        elif self.args.net == 'mtb':
            self.network = create_mtb(num_classes=4, depth=6)
            self.network.load_state_dict(torch.load('/data1/wangjingtao/workplace/python/pycharm_remote/meta-learning-classfication/pre_pth/mtb.pth'))
            print('using mtb with pretrain')     
            
            self.freeze_blocks_mtb()            

           
        self.network.train()
        self.network.cuda()
        return None

    def _init_opt(self):
        if self.args.inner_opt == 'SGD':
            self.inner_optimizer = torch.optim.SGD(self.network.parameters(), lr=self.args.inner_lr)
        elif self.args.inner_opt == 'Adam':
            self.inner_optimizer = torch.optim.Adam(self.network.parameters(), lr=self.args.inner_lr, betas=(0.0, 0.9))
        else:
            raise ValueError('Not supported inner optimizer.')
        if self.args.outer_opt == 'SGD':
            self.outer_optimizer = torch.optim.SGD(self.network.parameters(), lr=self.args.outer_lr, nesterov=True, momentum=0.9)
        elif self.args.outer_opt == 'Adam':
            self.outer_optimizer = torch.optim.Adam(self.network.parameters(), lr=self.args.outer_lr)
        else:
            raise ValueError('Not supported outer optimizer.')
        self.lr_scheduler = torch.optim.lr_scheduler.StepLR(self.outer_optimizer, step_size=10, gamma=0.5)
        return None

    # 换成自己的
    def unpack_batch_bak(self, batch):
        train_inputs, train_targets = batch['train']
        train_inputs = train_inputs.cuda()
        train_targets = train_targets.cuda()

        test_inputs, test_targets = batch['test']
        test_inputs = test_inputs.cuda()
        test_targets = test_targets.cuda()
        return train_inputs, train_targets, test_inputs, test_targets
    def unpack_batch(self, batch):
        device = torch.device('cuda')
        train_inputs, train_targets = batch[0]
        train_inputs = train_inputs.to(device=device)
        train_targets = train_targets.to(device=device, dtype=torch.long)

        test_inputs, test_targets = batch[1]
        test_inputs = test_inputs.to(device=device)
        test_targets = test_targets.to(device=device, dtype=torch.long)

        return train_inputs, train_targets, test_inputs, test_targets 

    def inner_loop(self):
        raise NotImplementedError

    def outer_loop(self):
        raise NotImplementedError

    def lr_sched(self):
        self.lr_scheduler.step()
        return None

    def load(self):
        path = os.path.join(self.args.result_path, self.args.alg, self.args.load_path)
        self.network.load_state_dict(torch.load(path))

    def load_encoder(self):
        path = os.path.join(self.args.result_path, self.args.alg, self.args.load_path)
        self.network.encoder.load_state_dict(torch.load(path))

    def save(self,filename):
        path = os.path.join(self.args.result_path, self.args.alg, filename)
        torch.save(self.network.state_dict(), path)

    def freeze_blocks(self):
        for param in self.network.parameters():
            param.requires_grad = False
        for i in [11]:
            for param in self.network.blocks[i].parameters():
                param.requires_grad = True

    def freeze_blocks_mtb(self):
        for param in self.network.parameters():
            param.requires_grad = False
        for param in self.network.meta_learner.parameters():
            param.requires_grad = True


